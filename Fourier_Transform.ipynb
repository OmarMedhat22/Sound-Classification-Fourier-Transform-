{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import glob\n",
    "import cv2\n",
    "from scipy.fftpack import fft,fftfreq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import pylab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import librosa    \n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "import keras\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = []\n",
    "labels = []\n",
    "sampling_rate = []\n",
    "file_names = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerate, data = wavfile.read(\"dataset/dog/1-30344-A.wav\")\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_sec = (len(data)/samplerate)\n",
    "step = time_sec/len(data)\n",
    "print(time_sec)\n",
    "print(step)\n",
    "i=0\n",
    "time_divion=[]\n",
    "while i<=time_sec-step:\n",
    "    \n",
    "    time_divion.append(i)\n",
    "    i=i+step\n",
    "# the fourth second step\n",
    "four_sec_step_number = (4*len(time_divion))/time_sec\n",
    "print(four_sec_step_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_number=0\n",
    "classes = []\n",
    "audio_data = []\n",
    "labels = []\n",
    "sampling_rate = []\n",
    "file_names = []\n",
    "data = []\n",
    "noisy_removed=[]\n",
    "noise=[]\n",
    "for filepath in glob.iglob('dataset/*'):\n",
    "    \n",
    "    #print(filepath[9:])\n",
    "    \n",
    "    #print(filepath)\n",
    "    classes.append(filepath[8:])\n",
    "\n",
    "print(classes)\n",
    "\n",
    "for i in classes:\n",
    "    print(\"the class = \"+i+\", the label = \"+str(label_number))\n",
    "    for j in glob.iglob('dataset/'+i+'/*'):\n",
    "        s, y = wavfile.read(j)\n",
    "        #y, s = librosa.load(j, sr=44000) # Downsample 44.1kHz to 8kHz\n",
    "        #reduced_noise = nr.reduce_noise(audio_clip=y, noise_clip=y, verbose=False)\n",
    "        #print(s)\n",
    "        #print(j)\n",
    "        data.append([y[:,0],label_number])\n",
    "        #noise.append(y)\n",
    "        #labels.append(label_number)\n",
    "        \n",
    "    label_number = label_number + 1\n",
    "print(len(labels))\n",
    "#print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(data)\n",
    "audio_data=[]\n",
    "labels=[]\n",
    "for i,j in data:\n",
    "    audio_data.append(i)\n",
    "    labels.append(j)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = random.randint(0,400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path='rain_before.jpg'\n",
    "plt.plot(time_divion[0:192000],audio_data[example][0:192000])\n",
    "#plt.show()\n",
    "plt.title('signal in real time')\n",
    "pylab.savefig(save_path, bbox_inches=None, pad_inches=0)\n",
    "pylab.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafft = fft(audio_data[example][0:192000])\n",
    "fft_sample = fft(audio_data[example][0:192000]) # take the fourier transform \n",
    "\n",
    "fftabs = abs(fft_sample)\n",
    "print(fftabs.shape)\n",
    "fftabs=fftabs/np.maximum.reduce(fftabs)\n",
    "freqs = fftfreq(len(audio_data[example][0:192000]),1/48000)\n",
    "freqs=freqs/np.maximum.reduce(freqs)\n",
    "\n",
    "##print(len(freqs))\n",
    "plt.plot(freqs[:len(fftabs)//2],fftabs[:len(fftabs)//2])\n",
    "#plt.plot(freqs,fftabs)\n",
    "#plt.show()\n",
    "\n",
    "save_path = 'rain.jpg'\n",
    "\n",
    "#plt.axis('off') # no axis\n",
    "plt.title('Fourier Transform')\n",
    "pylab.savefig(save_path, bbox_inches=None, pad_inches=0)\n",
    "pylab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_transform=[]\n",
    "for i in range(0,len(audio_data)):\n",
    "    \n",
    "    fast_f = fft(audio_data[i][0:192000]) # take the fourier transform \n",
    "    fft_abs = abs(fast_f)\n",
    "    fft_abs=fft_abs/np.maximum.reduce(fft_abs)\n",
    "    freqs = fftfreq(len(audio_data[i][0:192000]),1/48000)\n",
    "    freqs=freqs/np.maximum.reduce(freqs)\n",
    "    print(freqs.shape)\n",
    "    fourier_transform.append([fft_abs[:len(fft_abs)//2],freqs[:len(fft_abs)//2]])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(audio_data))\n",
    "print(len(fourier_transform))\n",
    "#print(len(audio_data))\n",
    "fourier_transform=np.array(fourier_transform)\n",
    "print(fourier_transform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_transform=fourier_transform.reshape(400,-1)\n",
    "fourier_transform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = fourier_transform\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(fourier_transform)\n",
    "\n",
    "\n",
    "print(np.amax(fourier_transform))\n",
    "\n",
    "normalized_features = scaler.transform(fourier_transform)\n",
    "print(np.amax(normalized_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X_train, X_test, y_train, y_test = train_test_split(fourier_transform, labels, test_size=0.20, random_state=1150)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_convolution = np.reshape(normalized_features,(400,1000, -1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=keras.utils.to_categorical(labels, num_classes=10, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), input_shape=features_convolution.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#'''\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "#'''\n",
    "#'''\n",
    "model.add(Conv2D(64, (3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#'''\n",
    "\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "#model.add(Dense(1000))#input_shape=features.shape[1:]\n",
    "model.add(Dense(64))#input_shape=features.shape[1:]\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "sgd = optimizers.SGD(lr=0.0000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(features_convolution, y,batch_size=1, epochs=40,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
